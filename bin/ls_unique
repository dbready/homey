#!/usr/bin/env python

# Print list of unique files as defined by a file hash (currently using blake2b)

import argparse
import hashlib
import os


def get_args():
    parser = argparse.ArgumentParser(
        description="Print the first found file with unique contents (defined by file blake2b hash) recursively under the src directory"
    )
    parser.add_argument("src", default=".", nargs='?')
    return parser.parse_args()


def hash_filename(filename):
    # algorithm choice arbitrary, but supposedly fast based on internet stranger
    hasher = hashlib.blake2b()
    with open(filename, "rb") as handle:
        chunk = handle.read(8192)
        while chunk:
            hasher.update(chunk)
            chunk = handle.read(8192)
    return hasher.hexdigest()


def gen_uniques(src):
    observed = set()
    for path, dirs, fnames in os.walk(src):
        for fname in sorted(fnames):
            filename = os.path.join(path, fname)
            # @TODO this is slow (reads all bytes from disk)
            # could imagine doing a two-pass system
            #   1) build list of unique file sizes
            #   2) if there is a collission on file size then hash file contents
            # would be more book-keeping, plus memory for (filesize, filepath, maybe[contents_hash])
            # but reading network/HDD, would be enormous time sizings for typical directory contents
            checksum = hash_filename(filename)
            if checksum not in observed:
                observed.add(checksum)
                yield filename


def main(src):
    filenames = gen_uniques(src)
    for filename in filenames:
        print(filename)


if __name__ == "__main__":
    args = get_args()
    src = os.path.normpath(args.src)
    main(src)
